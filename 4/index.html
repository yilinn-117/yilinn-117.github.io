<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Project 4A: Image Warping and Mosaicing</title>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 0;
            background-color: #f4f4f4;
        }
        header {
            background-color: #333;
            color: white;
            padding: 10px 0;
            text-align: center;
        }
        h1 {
            margin: 0;
            font-size: 36px;
        }
        section {
            margin: 20px;
            padding: 20px;
            background-color: white;
            border-radius: 8px;
        }
        .image-gallery {
            display: grid;
            grid-template-columns: repeat(5, 1fr); 
            gap: 6px; 
        }


        .image-gallery img {
            width: auto; 
            height: 100%; 
            display: block;
        }

        .image-container {
            display: flex;
            flex-wrap: wrap;
            text-align: center;
            width: 100%;
            gap: 5px;
        }
        .image-container img {
            width: 100%;
            height: auto;
            border-radius: 8px;
        }
        .image-container figcaption {
            margin-top: 5px;
            font-style: italic;
        }
        footer {
            background-color: #333;
            color: white;
            text-align: center;
            padding: 10px 0;
            margin-top: 20px;
        }
    </style>
</head>
<body>

<header>
    <h1>Project 4: [Auto]Stitching Photo Mosaics</h1>
</header>
    
<section>
    <h1>4A: Image Warping and Mosaicing</h1>
    <h2>1. Shoot the Pictures</h2>
    <h3>
        Those are the images I used in this project
    </h3>
    <h3>for Rectification:</h3>
    <div class="image-gallery">
        <figure class="image-container">
            <img src="./poster.jpg" alt="Original">
            <figcaption>Poster on Library Wall</figcaption>
        </figure>
        <figure class="image-container">
            <img src="./campanile.jpg" alt="D_x">
            <figcaption>Campanile</figcaption>
        </figure>
    </div>
    <h3>for Mosaicing:</h3>
    <div class="image-gallery">
        <figure class="image-container">
            <img src="./wurster_center.jpg" alt="Original">
            <figcaption>Wurster Hall</figcaption>
        </figure>
        <figure class="image-container">
            <img src="./wurster_up.jpg" alt="D_x">
            <figcaption></figcaption>
        </figure>
        <figure class="image-container">
            <img src="./fountain_down.jpg" alt="Original">
            <figcaption>Fountain</figcaption>
        </figure>
        <figure class="image-container">
            <img src="./fountain_up.jpg" alt="D_x">
            <figcaption></figcaption>
        </figure>
        <figure class="image-container">
            <img src="./EECS_left.jpg" alt="Original">
            <figcaption>EECS Hall</figcaption>
        </figure>
        <figure class="image-container">
            <img src="./EECS_right.jpg" alt="D_x">
            <figcaption></figcaption>
        </figure>
    </div>

    <h2>2. Recover Homographies</h2>
    <h2>Theoratical Backgroud</h3>
    <h3>We know that to do projective warping, we need to get the homography matrix H where Hp = p' and </h3>
    <p>
        \[
        H =
        \begin{bmatrix}
        a & b & c \\
        d & e & f \\
        g & h & 1
        \end{bmatrix}
        \]
    </p>
    <h3>H denotes the transformation</h3>
    <p>
        \[
        p =
        \begin{bmatrix}
        x \\
        y \\
        1
        \end{bmatrix}
        \]
    </p>
    <h3>p denotes a point in the base image</h3>
    <p>
        \[
        p' =
        \begin{bmatrix}
        wx' \\
        wy' \\
        w
        \end{bmatrix}
        \]        
    </p>
    <h3>p' denotes a point in the warped image</h3>
    <p>
        \[
        w =
        \begin{bmatrix}
        g & h & 1
        \end{bmatrix}
        \times
        \begin{bmatrix}
        x \\
        y \\
        1
        \end{bmatrix}
        \]        
    </p>
    <h3>w is calculated as this</h3>
    <h3>Substitude w in the above formula and expanding it, we found that coefficients in H could be expressed as</h3>
    <p>
        \[
        \begin{bmatrix}
        x & y & 1 & 0 & 0 & 0 & -xx' & -yx' \\
        0 & 0 & 0 & x & y & 1 & -xy' & -yy'
        \end{bmatrix}

        \begin{bmatrix}
        a \\
        b \\
        c \\
        d \\
        e \\
        f \\ 
        g \\
        h \\
        \end{bmatrix}
        =
        \begin{bmatrix}
        x' \\
        y'
        \end{bmatrix}
        \]
    </p>
    <h3>substitude each p & p' pairs and stack them in form of </h3>
    <p>
        \[
        \begin{bmatrix}
        x_1 & y_1 & 1 & 0 & 0 & 0 & -xx' & -yx' \\
        0 & 0 & 0 & x_1 & y_1 & 1 & -xy' & -yy' \\
        x_2 & y_2 & 1 & 0 & 0 & 0 & -xx' & -yx' \\
        0 & 0 & 0 & x_2 & y_2 & 1 & -xy' & -yy' \\
        e & t & c & e & t & c & . & .
        \end{bmatrix}

        \begin{bmatrix}
        a \\
        b \\
        c \\
        d \\
        e \\
        f \\ 
        g \\
        h \\
        \end{bmatrix}
        =
        \begin{bmatrix}
        x'_1 \\
        y'_1 \\
        x'_2 \\
        y'_2 \\
        . \\
        .
        \end{bmatrix}
        \]
    </p>
    <h3>We could solve for coefficients of H using least square where </h3>
    <p>
        \[
        x = (A^{t}A)^{-1} A^{t}b
        \]    
    </p>
    <h3>where</h3>
    <p>
        \[
        x =         
        \begin{bmatrix}
        a \\
        b \\
        c \\
        d \\
        e \\
        f \\ 
        g \\
        h \\
        \end{bmatrix}
        \]    
    </p>
    <p>
        \[
        A =         
        \begin{bmatrix}
        x_1 & y_1 & 1 & 0 & 0 & 0 & -xx' & -yx' \\
        0 & 0 & 0 & x_1 & y_1 & 1 & -xy' & -yy' \\
        x_2 & y_2 & 1 & 0 & 0 & 0 & -xx' & -yx' \\
        0 & 0 & 0 & x_2 & y_2 & 1 & -xy' & -yy' \\
        e & t & c & e & t & c & . & .
        \end{bmatrix}
        \]    
    </p>
    <p>
        \[
        b =         
        \begin{bmatrix}
        x'_1 \\
        y'_1 \\
        x'_2 \\
        y'_2 \\
        . \\
        .
        \end{bmatrix}
        \]    
    </p>

    <h2>3. Warping the Images</h2>
    <h3>
        First thing to do in warping is to calculate the transformation matrix H, which needs correspondence points p p' pairs to do:
    </h3>
    <h3>Below are the correspondence I annotated</h3>
    <div class="image-gallery">
        <figure class="image-container">
            <img src="./wurster_pts1.png" alt="Offset: r[90,22], g[40,16]">
            <figcaption></figcaption>
        </figure>
        <figure class="image-container">
            <img src="./wurster_pts2.png" alt="Offset: r[90,22], g[40,16]">
            <figcaption></figcaption>
        </figure>
    </div>
    <div class="image-gallery">
        <figure class="image-container">
            <img src="./EECSleft_pts.png" alt="Original">
            <figcaption></figcaption>
        </figure>
        <figure class="image-container">
            <img src="./EECSright_pts.png" alt="Original">
            <figcaption></figcaption>
        </figure>
    </div>
    <div class="image-gallery">
        <figure class="image-container">
            <img src="./fountaindown_pts.png" alt="Offset: r[90,22], g[40,16]">
            <figcaption></figcaption>
        </figure>
        <figure class="image-container">
            <img src="./fountainup_pts.png" alt="Offset: r[90,22], g[40,16]">
            <figcaption></figcaption>
        </figure>
    </div>
    <h3> 
        The second step to do warping is to calculate the coordinate of warped image, where I do by first apply the calculated H to the four corners of the original 
        rectangular image to find the polygon shape where the warp is going to take form. Then, I use inverse warping: I loop over each of the points in the final polygon
        and calculate \p = wH^{-1}p'\ as the point on the original image to do interpolation for color and feed the color back into the final polygon.
    </h3>
    <h3>
        Below are the warped image of wurster hall, fountain and EECS hall:
    </h3>
    <div class="image-gallery">
        <figure class="image-container">
            <img src="./wurster_warp.png" alt="Original">
            <figcaption>Warped high perspective of Wurster</figcaption>
        </figure>
        <figure class="image-container">
            <img src="./fountain_warp.png" alt="Original">
            <figcaption>Warped high perspective of fountain</figcaption>
        </figure>
        <figure class="image-container">
            <img src="./EECS_warp.png" alt="Original">
            <figcaption>Warped right perspective of EECS Hall</figcaption>
        </figure>
    </div>

    <h2>4. Rectify Images</h2>
    <h3>
        After implementing the warping function, we could rectify an image: simply annotate a known rectangle in an image and warp it toward a 
        set of 4 points which compose a rectangle. The set of 4 points are chosen by hand, but their value should be adjusted to preserve original resolution.
    </h3>
    <h3>Below are the annotated images and their rectifications, the original image could be found in section 'Shoot the Pictures' above</h3>
    <div class="image-gallery">
        <figure class="image-container">
            <img src="./poster_pts.png" alt="Offset: r[90,22], g[40,16]">
            <figcaption>Annotated poster</figcaption>
        </figure>
        <figure class="image-container">
            <img src="./trans_poster.png" alt="Offset: r[90,22], g[40,16]">
            <figcaption>Rectified poster</figcaption>
        </figure>
    </div>
    <div class="image-gallery">
        <figure class="image-container">
            <img src="./campanile_pts.png" alt="Original">
            <figcaption>Annotated campanile</figcaption>
        </figure>
        <figure class="image-container">
            <img src="./trans_campanile.png" alt="Original">
            <figcaption>Rectified campanile</figcaption>
        </figure>
    </div>

    <h2>5. Blending into Mosaic</h2>
    <h3>
        After obtaining the warped image, there are two more steps before we could morph warped and original into a mosaic, 
        the two steps are alignment and blending.
    </h3>
    <h3>I. Alignment</h3>
    <h3>
        To do alignment between the original and the warped image. I obtain a mean point by average between the
        annotated points in base image and transformed annotated point in the warped image. Then I stack the mean point and calculate
        the width and height of the final mosaic image by finding the maximum offset between mean point and each of the 4 boundries of original and warped.
    </h3>

    <h3>Below is the example of alignment on Wurster hall images, the green point denotes the mean point, the gray part is occupied by one of the images, black occupied
        by none and white is the intersection.
    </h3>

    <div class="image-gallery">
        <figure class="image-container">
            <img src="./align_example.png" alt="Offset: r[90,22], g[40,16]">
            <figcaption>Meaning point of original</figcaption>
        </figure>
        <figure class="image-container">
            <img src="./align_example1.png" alt="Offset: r[90,22], g[40,16]">
            <figcaption>Transformed Meaning point of warped</figcaption>
        </figure>
        <figure class="image-container">
            <img src="./alignment_sample.png" alt="Offset: r[90,22], g[40,16]">
            <figcaption>Mosaic Dimension Demonstration</figcaption>
        </figure>
    </div>
    <h3>II. Blending</h3>
    <h3>
        To do blending, I implemented an alpha mask on the obtained interection in the alignment process.
    </h3>

    <h3>Below is the vertical alpha mask used on Wurster/fountain images, and the horizontal alpha mask used on the EECS images.</p>

    <div class="image-gallery">
        <figure class="image-container">
            <img src="./vertical_mask.png" alt="Offset: r[90,22], g[40,16]">
            <figcaption>Vertical Alpha Mask</figcaption>
        </figure>
        <figure class="image-container">
            <img src="./vertical_mask2.png" alt="Offset: r[90,22], g[40,16]">
            <figcaption></figcaption>
        </figure>
    </div>
    <div class="image-gallery">
        <figure class="image-container">
            <img src="./horizontal_mask1.png" alt="Offset: r[90,22], g[40,16]">
            <figcaption>Horizontal Alpha Mask</figcaption>
        </figure>
        <figure class="image-container">
            <img src="./horizontal_mask2.png" alt="Offset: r[90,22], g[40,16]">
            <figcaption></figcaption>
        </figure>
    </div>
    <h3>6. The Mosaic</h3>
    <h3>
        After doing alignment and blending, we could finally create mosaics, below are the mosaics of wurster, fountain and EECS hall, inaccompany of the original images.
    </h3>
    <div class="image-gallery">
        <figure class="image-container">
            <img src="./wurster_center.jpg" alt="Offset: r[90,22], g[40,16]">
            <figcaption>Original 1</figcaption>
        </figure>
        <figure class="image-container">
            <img src="./wurster_up.jpg" alt="Offset: r[90,22], g[40,16]">
            <figcaption>Original 2</figcaption>
        </figure>
        <figure class="image-container">
            <img src="./trans_wurster.png" alt="Offset: r[90,22], g[40,16]">
            <figcaption>Mosaic</figcaption>
        </figure>
    </div>
    <div class="image-gallery">
        <figure class="image-container">
            <img src="./fountain_down.jpg" alt="Offset: r[90,22], g[40,16]">
            <figcaption>Original 1</figcaption>
        </figure>
        <figure class="image-container">
            <img src="./fountain_up.jpg" alt="Offset: r[90,22], g[40,16]">
            <figcaption>Original 2</figcaption>
        </figure>
        <figure class="image-container">
            <img src="./trans_fountain.png" alt="Offset: r[90,22], g[40,16]">
            <figcaption>Mosaic</figcaption>
        </figure>
    </div>
    <div class="image-gallery">
        <figure class="image-container">
            <img src="./EECS_left.jpg" alt="Offset: r[90,22], g[40,16]">
            <figcaption>Original 1</figcaption>
        </figure>
        <figure class="image-container">
            <img src="./EECS_right.jpg" alt="Offset: r[90,22], g[40,16]">
            <figcaption>Original 2</figcaption>
        </figure>
        <figure class="image-container">
            <img src="./trans_EECS.png" alt="Offset: r[90,22], g[40,16]">
            <figcaption>Mosaic</figcaption>
        </figure>
    </div>

    <h2>Conclusion for 4A</h2>
    <h3>
        This is a very interesting project, it enables me to transform an image to achieve a semi-change of perspective, which I used to thought was impossible
        since it would be including extra information. It is amazing and satisfying to know that there are tricks we could do to trick our eyes, like projecting original information on a new projection plane.
    </h3>

</section>
<section>
    <h1>4B: Feature Matching for Autostitching</h1>
    <h2>1. Detecting corner features in an image</h2>
    <h3>
        To find the corners in an image, I took Harris' Corner Detector approach.
        I used the provided function get_harris_corners() in harris.py to do so, below is the visual result of 
        this function on the wurster hall picture, where h value correspond to the 'corner-ness' of a pixel:
    </h3>
    <div class="image-gallery">
        <figure class="image-container">
            <img src="./hmap.png">
            <figcaption>The h value for every pixel </figcaption>
        </figure>
        <figure class="image-container">
            <img src="./harriscorrner.png">
            <figcaption>Each detected 'Corner', with an h value > 0</figcaption>
        </figure>
    </div>
    <h3>
        It could be seen that they are spatially even across the image, but they are too dense which would lead to very expensive (and unnecessary) computations.
        Therefore, I need to filter them while keeping them spatially even, I used Adaptive Non-maximal Suppression method mentioned in the Brown et al. paper.
        Since we need to calculate the distance between each point to each point, it is computationally expensive to do,
        but since my picture is small (800x600), where the harris corner is around ~10000 for each picture, using the provided dist2() function is sufficient.
    </h3>
    <div class="image-gallery">
        <figure class="image-container">
            <img src="./anmscorner.png">
            <figcaption>Corners after ANMS</figcaption>
        </figure>
    </div>
    <h3>
        We could see that ANMS preserves spatially evenness while reducing the density of corners,
        saving a lot of computation for future homography.
    </h3>

    <h2>2. Extracting a Feature Descriptor for each feature point</h2>
    <h3>
        After we got a decent amount of corners in our target image, we need to extract 
        a feature descriptor for each point so that we could compare similarity between two points.
        For each target point, I sample a 8x8 patch from the 40x40 pixels around the target point to get a blurred descriptor.
        Then I normalize the descriptors, following the steps described in the paper.
        Below are some examples of features, sampled patch and normalized patch:
    </h3>
    <div class="image-gallery">
        <figure class="image-container">
            <img src="./feature1.png">
            <figcaption>Feature 18</figcaption>
        </figure>
        <figure class="image-container">
            <img src="./feature2.png">
            <figcaption>Feature 40</figcaption>
        </figure>
        <figure class="image-container">
            <img src="./feature3.png">
            <figcaption>Feature 2</figcaption>
        </figure>
    </div>
    <h2>3. Matching these feature descriptors between two images</h2>
    <h3>
        After collecting feature for each target points, we need to do matching between the features of two images.
        I need to find features that look similar to identify if they are a good match and could be used to calculate a homography.
        We know that for each point (that has a match), there is only one correct match, which suggests that if the error ratio between one point's closest match and second closest match is close to 1,
        this match is probably fake. Using this Lowe's trick, I set the threshold ratio to be 0.4, if the error of a point's best match is more than 0.4 of the point's second best match, 
        we just identify this point as no match and discard it. Below is the matching result using Lowe's trick for all 3 mosaics:
    </h3>
    <div class="image-gallery">
        <figure class="image-container">
            <img src="./wurstercorner.png">
            <figcaption>Lowe Filtered Matches for Wurster</figcaption>
        </figure>
        <figure class="image-container">
            <img src="./fountaincorner.png">
            <figcaption>Lowe Filtered Matches for Fountain</figcaption>
        </figure>
    </div>
    <div class="image_gallery">
        <img src="./eecscorner.png">
        <figcaption>Lowe Filtered Matches for EECS</figcaption>
    </div>
    <h2>4. Use a robust method (RANSAC) to compute a homograph</h2>
    <h3>
        We could see that while most of the point pairs appear to be valid, there are some mismatches.
        Since we are computing homogrphy with least squares, several outliers could sabotage our projective matrix.
        Hence, we need to use RANSAC to select a sub-set of consensus point matches to calculate our homography, and reject the outliers.
        To do so, we select four feature pairs (minimum for a homography), calculate a homography, and count the inliers where Hp is very close to p'. 
        We do this steps for a preset amount of times, find the group with the most inliers and compute a homogrphy according to only them.
        Below is the result after applying RANSAC.
    </h3>
    <div class="image-gallery">
        <figure class="image-container">
            <img src="./wursterransaccorner.png">
            <figcaption>RANSAC Matches for Wurster</figcaption>
        </figure>
        <figure class="image-container">
            <img src="./fountainransaccorner.png">
            <figcaption>RANSAC Filtered Matches for Fountain</figcaption>
        </figure>
    </div>
    <div class="image_gallery">
        <img src="./eecsransaccorner.png">
        <figcaption>RANSAC Filtered Matches for EECS</figcaption>
    </div>
    <h3>
        We could observe that while the number of pairs is reduced, all of them appear to be valid. 
        RANSAC is working well.
    </h3>
    <h2>5. Use the auto detected point to produce a mosaic</h2>
    <h3>
        Below is the result comparison between hand-annotated and auto-stitching.
    </h3>
    <div class="image-gallery">
        <figure class="image-container">
            <img src="./trans_wurster.png">
            <figcaption>Hand annotated result for Wurster</figcaption>
        </figure>
        <figure class="image-container">
            <img src="./Wurster_auto.png">
            <figcaption>Auto-stitching result for Wurster</figcaption>
        </figure>
    </div>
    <div class="image-gallery">
        <figure class="image-container">
            <img src="./trans_fountain.png">
            <figcaption>Hand annotated result for Fountain</figcaption>
        </figure>
        <figure class="image-container">
            <img src="./fountain_auto.png">
            <figcaption>Auto-stitching result for Fountain</figcaption>
        </figure>
    </div>
    <div class="image-gallery">
        <figure class="image-container">
            <img src="./trans_EECS.png">
            <figcaption>Hand annotated result for EECS</figcaption>
        </figure>
        <figure class="image-container">
            <img src="./EECS_auto.png">
            <figcaption>Auto-stitching result for EECS</figcaption>
        </figure>
    </div>
    <h3>
        We could see that the auto-stitching result is equally good and even better (like the fountain example) than hand-annotated result.
        This proves that the algorithm for auto-stitching, while old, is still very effective and amazing.
    </h3>
    <h2>Conclusion for 4B</h2>
    <h3>
        This project is very amazing, the ANMS and RANSAC algorithm are new for me and I think this way of solving the problem could be
        used in other fields. Also, this project provides me a bit of insight of some of the comtemporary visual neural networks, 
        while much more advanced, they are still doing the same thing as a algorithm that could do 20 years ago: detecting edges.
    </h3>
</section>


<footer>
    <p>&copy; Yilin Ni's Project 4</p>
</footer>

</body>
</html>